from search import search, load_embeddings
from sentence_transformers import SentenceTransformer
import numpy as np
import json
import os
from openai import OpenAI

# ================== CONFIGURA√á√ïES ==================

USE_LOCAL_LLM = False  # False = OpenAI | True = Ollama

MIN_RAG_SCORE = 0.60
MIN_INTENT_SCORE = 0.65

# ==================================================

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
model = SentenceTransformer("all-MiniLM-L6-v2")


# ================== LLM ROUTER ==================

def call_llm(prompt):
    """
    Roteador central de LLM
    """
    if USE_LOCAL_LLM:
        return call_llm_local(prompt)
    return call_llm_openai(prompt)


# ================== PLANO A - OPENAI ==================

def call_llm_openai(prompt):
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "Voc√™ responde apenas em portugu√™s brasileiro."},
            {"role": "user", "content": prompt}
        ],
        temperature=0
    )
    return response.choices[0].message.content


# ================== PLANO B - LLM LOCAL (OLLAMA) ==================
# 1¬∞ Descomente o c√≥digo abaixo
# 2Ô∏è¬∞ Instale requests
# 3Ô∏è¬∞ Altere USE_LOCAL_LLM = True
"""
import requests

def call_llm_local(prompt):
    response = requests.post(
        "http://localhost:11434/api/generate",
        json={
            "model": "mistral",
            "prompt": prompt,
            "stream": False
        }
    )
    data = response.json()
    return data["response"]
"""

# Fallback seguro caso USE_LOCAL_LLM=True sem ativar Ollama
def call_llm_local(prompt):
    raise RuntimeError(
        "LLM local n√£o est√° ativa. "
        "Descomente o c√≥digo do Ollama e configure corretamente."
    )


# ================== PROMPTS ==================

def build_rag_prompt(context, question):
    return f"""
Voc√™ √© um assistente que responde APENAS em portugu√™s brasileiro.

Responda SOMENTE com base no contexto abaixo.
N√£o utilize nenhum conhecimento externo.

Contexto:
{context}

Pergunta:
{question}

Resposta:
"""


def build_general_prompt(question):
    return f"""
Voc√™ √© um assistente em portugu√™s brasileiro.

Responda a pergunta abaixo usando seu conhecimento geral.

Pergunta:
{question}

Resposta:
"""


# ================== INTENTS (Verifica√ß√£o s√™mantica caso o usu√°rio queira o contexto/fonte da resposta) ==================

def load_intents():
    with open("data/intents.json", encoding="utf-8") as f:
        return json.load(f)


def detect_intent(question, intents):
    question_emb = model.encode([question])[0]

    best_intent = None
    best_score = 0

    for intent_name, data in intents.items():
        for emb in data["embeddings"]:
            emb = np.array(emb)
            score = np.dot(question_emb, emb) / (
                np.linalg.norm(question_emb) * np.linalg.norm(emb)
            )

            if score > best_score:
                best_score = score
                best_intent = intent_name

    if best_score >= MIN_INTENT_SCORE:
        return best_intent, best_score

    return None, best_score


# ================== MAIN ==================

if __name__ == "__main__":
    texts, embeddings = load_embeddings()
    intents = load_intents()

    while True:
        question = input("\nPergunta (ou 'sair'): ")
        if question.lower() == "sair":
            break

        # 1Ô∏è‚É£ Detectar inten√ß√£o
        intent, intent_score = detect_intent(question, intents)

        # 2Ô∏è‚É£ Recupera√ß√£o RAG
        results = search(question, texts, embeddings, top_k=1)

        # 3Ô∏è‚É£ RAG fraco ‚Üí conhecimento geral da LLM
        if not results or results[0]["score"] < MIN_RAG_SCORE:
            prompt = build_general_prompt(question)
            answer = call_llm(prompt)

            print("\nüß† Resposta:")
            print(answer)
            continue

        context = results[0]["text"]

        # 4Ô∏è‚É£ Usu√°rio pediu explicitamente o contexto
        if intent == "show_context":
            print("\nüìÑ Contexto utilizado:")
            print(context)
            continue

        # 5Ô∏è‚É£ Prompt RAG
        prompt = build_rag_prompt(context, question)

        # 6Ô∏è‚É£ Chamada da LLM
        answer = call_llm(prompt)

        print("\nüß† Resposta:")
        print(answer)
